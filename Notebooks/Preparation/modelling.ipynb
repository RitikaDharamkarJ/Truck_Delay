{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1074326\n",
      "Successfully connected to Hopsworks.\n",
      "2024-10-30 10:56:40,972 WARNING: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead\n",
      "\n",
      "2024-10-30 10:56:40,973 WARNING: using legacy validation callback\n",
      "Connected. Call `.close()` to terminate connection gracefully.\n",
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (1.61s) \n",
      "Downloaded feature group: final_merge_df (version 1)\n",
      "   truck_id    route_id            departure_date         estimated_arrival  \\\n",
      "0  21999423  R-65cfb635 2019-01-01 07:00:00+00:00 2019-01-01 12:00:00+00:00   \n",
      "1  25951646  R-05f9858a 2019-01-10 07:00:00+00:00 2019-01-11 01:00:00+00:00   \n",
      "2  31766919  R-08c77b90 2019-01-26 07:00:00+00:00 2019-01-27 08:00:00+00:00   \n",
      "3  33921946  R-5cc73966 2019-01-16 07:00:00+00:00 2019-01-16 08:00:00+00:00   \n",
      "4  13792231  R-71af6d34 2019-01-13 07:00:00+00:00 2019-01-13 18:00:00+00:00   \n",
      "\n",
      "   delay  route_avg_temp  route_avg_wind_speed  route_avg_precip  \\\n",
      "0      0       48.000000              8.000000               0.0   \n",
      "1      0       48.400000              6.200000               0.0   \n",
      "2      0       36.833333             15.166667               0.0   \n",
      "3      0       58.000000              7.000000               0.0   \n",
      "4      0       40.000000              6.000000               0.0   \n",
      "\n",
      "   route_avg_humidity  route_avg_visibility  ...  gender   age experience  \\\n",
      "0           58.500000                   5.0  ...    male  40.0        8.0   \n",
      "1           49.000000                   6.0  ...    male  39.0        4.0   \n",
      "2           56.333333                   6.0  ...    male  51.0       15.0   \n",
      "3           54.500000                   6.0  ...    male  45.0       16.0   \n",
      "4           84.333333                   6.0  ...    male  45.0        8.0   \n",
      "\n",
      "  driving_style ratings  vehicle_no  average_speed_mph  is_midnight  \\\n",
      "0  conservative     7.0  21999423.0              46.78            0   \n",
      "1  conservative     2.0  25951646.0              47.45            1   \n",
      "2  conservative     8.0  31766919.0              43.41            1   \n",
      "3  conservative     7.0  33921946.0              36.30            0   \n",
      "4     proactive     7.0  13792231.0              60.53            0   \n",
      "\n",
      "   unique_id                event_time  \n",
      "0      11750 2024-10-08 00:00:00+00:00  \n",
      "1      10226 2024-10-08 00:00:00+00:00  \n",
      "2       2921 2024-10-08 00:00:00+00:00  \n",
      "3         96 2024-10-08 00:00:00+00:00  \n",
      "4       4167 2024-10-08 00:00:00+00:00  \n",
      "\n",
      "[5 rows x 50 columns]\n"
     ]
    }
   ],
   "source": [
    "import hopsworks\n",
    "from hsfs.client.exceptions import RestAPIError\n",
    "\n",
    "try:\n",
    "    # Establish connection to Hopsworks\n",
    "    project = hopsworks.login(\n",
    "        api_key_value=\"O4IOxWozstKu0BFQ.07C1tbvgVI5C4XNLbLrGH4PS4t0EqBYN00ex8318TNIkl82WwDi3Vh9MidMrCA83\"\n",
    "    )\n",
    "    print(\"Successfully connected to Hopsworks.\")\n",
    "except Exception as e:\n",
    "    print(\"Failed to connect to Hopsworks.\")\n",
    "    print(e)\n",
    "    exit(1)  # Exit if the connection fails\n",
    "\n",
    "try:\n",
    "    # Access the Feature Store\n",
    "    fs = project.get_feature_store()\n",
    "\n",
    "    # Retrieve the feature group by name and version\n",
    "    feature_group = fs.get_feature_group(\"final_merge_df\", version=1)\n",
    "\n",
    "    # Read the feature group as a DataFrame\n",
    "    df = feature_group.read()\n",
    "\n",
    "    # Print confirmation and the first few rows of the DataFrame\n",
    "    print(\"Downloaded feature group: final_merge_df (version 1)\")\n",
    "    print(df.head())\n",
    "\n",
    "except RestAPIError as e:\n",
    "    print(\"Error downloading feature group: final_merge_df (version 1)\")\n",
    "    print(e)\n",
    "except Exception as e:\n",
    "    print(\"An unexpected error occurred.\")\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "truck_id                          0\n",
       "route_id                          0\n",
       "departure_date                    0\n",
       "estimated_arrival                 0\n",
       "delay                             0\n",
       "route_avg_temp                    0\n",
       "route_avg_wind_speed              0\n",
       "route_avg_precip                  0\n",
       "route_avg_humidity                0\n",
       "route_avg_visibility              0\n",
       "route_avg_pressure                0\n",
       "route_description                 0\n",
       "estimated_arrival_nearest_hour    0\n",
       "departure_date_nearest_hour       0\n",
       "origin_id                         0\n",
       "destination_id                    0\n",
       "distance                          0\n",
       "average_hours                     0\n",
       "temp_x                            0\n",
       "wind_speed_x                      0\n",
       "description_x                     0\n",
       "precip_x                          0\n",
       "humidity_x                        0\n",
       "visibility_x                      0\n",
       "pressure_x                        0\n",
       "temp_y                            0\n",
       "wind_speed_y                      0\n",
       "description_y                     0\n",
       "precip_y                          0\n",
       "humidity_y                        0\n",
       "visibility_y                      0\n",
       "pressure_y                        0\n",
       "avg_no_of_vehicles                0\n",
       "accident                          0\n",
       "truck_age                         0\n",
       "load_capacity_pounds              0\n",
       "mileage_mpg                       0\n",
       "fuel_type                         0\n",
       "driver_id                         0\n",
       "name                              0\n",
       "gender                            0\n",
       "age                               0\n",
       "experience                        0\n",
       "driving_style                     0\n",
       "ratings                           0\n",
       "vehicle_no                        0\n",
       "average_speed_mph                 0\n",
       "is_midnight                       0\n",
       "unique_id                         0\n",
       "event_time                        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the date range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_merge = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Timestamp('2019-01-01 07:00:00+0000', tz='UTC'),\n",
       " Timestamp('2019-02-13 06:00:00+0000', tz='UTC'))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_merge['estimated_arrival'].min(), final_merge['estimated_arrival'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the data into training, validation, and test sets based on date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['truck_id', 'route_id', 'departure_date', 'estimated_arrival', 'delay',\n",
       "       'route_avg_temp', 'route_avg_wind_speed', 'route_avg_precip',\n",
       "       'route_avg_humidity', 'route_avg_visibility', 'route_avg_pressure',\n",
       "       'route_description', 'estimated_arrival_nearest_hour',\n",
       "       'departure_date_nearest_hour', 'origin_id', 'destination_id',\n",
       "       'distance', 'average_hours', 'temp_x', 'wind_speed_x', 'description_x',\n",
       "       'precip_x', 'humidity_x', 'visibility_x', 'pressure_x', 'temp_y',\n",
       "       'wind_speed_y', 'description_y', 'precip_y', 'humidity_y',\n",
       "       'visibility_y', 'pressure_y', 'avg_no_of_vehicles', 'accident',\n",
       "       'truck_age', 'load_capacity_pounds', 'mileage_mpg', 'fuel_type',\n",
       "       'driver_id', 'name', 'gender', 'age', 'experience', 'driving_style',\n",
       "       'ratings', 'vehicle_no', 'average_speed_mph', 'is_midnight',\n",
       "       'unique_id', 'event_time'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_merge.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Column Names:\n",
      "Index(['truck_id', 'route_id', 'departure_date', 'estimated_arrival', 'delay',\n",
      "       'route_avg_temp', 'route_avg_wind_speed', 'route_avg_precip',\n",
      "       'route_avg_humidity', 'route_avg_visibility', 'route_avg_pressure',\n",
      "       'route_description', 'estimated_arrival_nearest_hour',\n",
      "       'departure_date_nearest_hour', 'origin_id', 'destination_id',\n",
      "       'distance', 'average_hours', 'temp_origin', 'wind_speed_origin',\n",
      "       'description_origin', 'precip_origin', 'humidity_origin',\n",
      "       'visibility_origin', 'pressure_origin', 'temp_destination',\n",
      "       'wind_speed_destination', 'description_destination',\n",
      "       'precip_destination', 'humidity_destination', 'visibility_destination',\n",
      "       'pressure_destination', 'avg_no_of_vehicles', 'accident', 'truck_age',\n",
      "       'load_capacity_pounds', 'mileage_mpg', 'fuel_type', 'driver_id', 'name',\n",
      "       'gender', 'age', 'experience', 'driving_style', 'ratings', 'vehicle_no',\n",
      "       'average_speed_mph', 'is_midnight', 'unique_id', 'event_time'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Ensure the suffixes '_x' and '_y' are replaced with '_origin' and '_destination'\n",
    "final_merge.columns = final_merge.columns.str.replace('_x$', '_origin', regex=True)\n",
    "final_merge.columns = final_merge.columns.str.replace('_y$', '_destination', regex=True)\n",
    "\n",
    "# Optional: Verify the changes\n",
    "print(\"Updated Column Names:\")\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updated Code to Rename Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Column Names:\n",
      "Index(['truck_id', 'route_id', 'departure_date', 'estimated_arrival', 'delay',\n",
      "       'route_avg_temp', 'route_avg_wind_speed', 'route_avg_precip',\n",
      "       'route_avg_humidity', 'route_avg_visibility', 'route_avg_pressure',\n",
      "       'route_description', 'estimated_arrival_nearest_hour',\n",
      "       'departure_date_nearest_hour', 'origin_id', 'destination_id',\n",
      "       'distance', 'average_hours', 'temp_origin', 'wind_speed_origin',\n",
      "       'origin_description', 'precip_origin', 'humidity_origin',\n",
      "       'visibility_origin', 'pressure_origin', 'temp_destination',\n",
      "       'wind_speed_destination', 'destination_description',\n",
      "       'precip_destination', 'humidity_destination', 'visibility_destination',\n",
      "       'pressure_destination', 'avg_no_of_vehicles', 'accident', 'truck_age',\n",
      "       'load_capacity_pounds', 'mileage_mpg', 'fuel_type', 'driver_id', 'name',\n",
      "       'gender', 'age', 'experience', 'driving_style', 'ratings', 'vehicle_no',\n",
      "       'average_speed_mph', 'is_midnight', 'unique_id', 'event_time'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Dictionary mapping actual column names to expected column names\n",
    "rename_mapping = {\n",
    "    'origin_temp': 'temp_origin',\n",
    "    'origin_wind_speed': 'wind_speed_origin',\n",
    "    'origin_precip': 'precip_origin',\n",
    "    'origin_humidity': 'humidity_origin',\n",
    "    'origin_visibility': 'visibility_origin',\n",
    "    'origin_pressure': 'pressure_origin',\n",
    "    'destination_temp': 'temp_destination',\n",
    "    'destination_wind_speed': 'wind_speed_destination',\n",
    "    'destination_precip': 'precip_destination',\n",
    "    'destination_humidity': 'humidity_destination',\n",
    "    'destination_visibility': 'visibility_destination',\n",
    "    'destination_pressure': 'pressure_destination',\n",
    "    'description_origin': 'origin_description',\n",
    "    'description_destination': 'destination_description'\n",
    "}\n",
    "\n",
    "# Apply the renaming to the DataFrame\n",
    "final_merge = final_merge.rename(columns=rename_mapping)\n",
    "\n",
    "# Optional: Verify the changes\n",
    "print(\"Updated Column Names:\")\n",
    "print(final_merge.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cts_cols=['route_avg_temp', 'route_avg_wind_speed',\n",
    "              'route_avg_precip', 'route_avg_humidity', 'route_avg_visibility',\n",
    "              'route_avg_pressure', 'distance', 'average_hours',\n",
    "              'origin_temp', 'origin_wind_speed', 'origin_precip', 'origin_humidity',\n",
    "              'origin_visibility', 'origin_pressure',\n",
    "              'destination_temp','destination_wind_speed','destination_precip',\n",
    "              'destination_humidity', 'destination_visibility','destination_pressure',\n",
    "               'avg_no_of_vehicles', 'truck_age','load_capacity_pounds', 'mileage_mpg',\n",
    "               'age', 'experience','average_speed_mph']\n",
    "\n",
    "cat_cols=['route_description',\n",
    "              'origin_description', 'destination_description',\n",
    "               'accident', 'fuel_type',\n",
    "              'gender', 'driving_style', 'ratings','is_midnight']\n",
    "\n",
    "target=['delay']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Timestamp('2019-01-01 07:00:00+0000', tz='UTC'),\n",
       " Timestamp('2019-02-13 06:00:00+0000', tz='UTC'))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking the date range\n",
    "final_merge['estimated_arrival'].min(), final_merge['estimated_arrival'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the data into training, validation, and test sets based on date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Ensure 'estimated_arrival' is timezone-aware (UTC)\n",
    "final_merge['estimated_arrival'] = final_merge['estimated_arrival'].dt.tz_convert('UTC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   truck_id    route_id            departure_date         estimated_arrival  \\\n",
      "0  21999423  R-65cfb635 2019-01-01 07:00:00+00:00 2019-01-01 12:00:00+00:00   \n",
      "1  25951646  R-05f9858a 2019-01-10 07:00:00+00:00 2019-01-11 01:00:00+00:00   \n",
      "2  31766919  R-08c77b90 2019-01-26 07:00:00+00:00 2019-01-27 08:00:00+00:00   \n",
      "3  33921946  R-5cc73966 2019-01-16 07:00:00+00:00 2019-01-16 08:00:00+00:00   \n",
      "4  13792231  R-71af6d34 2019-01-13 07:00:00+00:00 2019-01-13 18:00:00+00:00   \n",
      "\n",
      "   delay  route_avg_temp  route_avg_wind_speed  route_avg_precip  \\\n",
      "0      0       48.000000              8.000000               0.0   \n",
      "1      0       48.400000              6.200000               0.0   \n",
      "2      0       36.833333             15.166667               0.0   \n",
      "3      0       58.000000              7.000000               0.0   \n",
      "4      0       40.000000              6.000000               0.0   \n",
      "\n",
      "   route_avg_humidity  route_avg_visibility  ...  gender   age experience  \\\n",
      "0           58.500000                   5.0  ...    male  40.0        8.0   \n",
      "1           49.000000                   6.0  ...    male  39.0        4.0   \n",
      "2           56.333333                   6.0  ...    male  51.0       15.0   \n",
      "3           54.500000                   6.0  ...    male  45.0       16.0   \n",
      "4           84.333333                   6.0  ...    male  45.0        8.0   \n",
      "\n",
      "  driving_style ratings  vehicle_no  average_speed_mph  is_midnight  \\\n",
      "0  conservative     7.0  21999423.0              46.78            0   \n",
      "1  conservative     2.0  25951646.0              47.45            1   \n",
      "2  conservative     8.0  31766919.0              43.41            1   \n",
      "3  conservative     7.0  33921946.0              36.30            0   \n",
      "4     proactive     7.0  13792231.0              60.53            0   \n",
      "\n",
      "   unique_id                event_time  \n",
      "0      11750 2024-10-08 00:00:00+00:00  \n",
      "1      10226 2024-10-08 00:00:00+00:00  \n",
      "2       2921 2024-10-08 00:00:00+00:00  \n",
      "3         96 2024-10-08 00:00:00+00:00  \n",
      "4       4167 2024-10-08 00:00:00+00:00  \n",
      "\n",
      "[5 rows x 50 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create a UTC-aware comparison timestamp\n",
    "comparison_date = pd.to_datetime('2019-01-30', utc=True)\n",
    "\n",
    "# Filter the DataFrame based on the comparison\n",
    "train_df = final_merge[final_merge['estimated_arrival'] <= comparison_date]\n",
    "\n",
    "# Optional: Display the result to verify\n",
    "print(train_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create UTC-aware comparison timestamps\n",
    "start_date = pd.to_datetime('2019-01-30', utc=True)\n",
    "end_date = pd.to_datetime('2019-02-07', utc=True)\n",
    "\n",
    "# Filter the DataFrame based on the date range\n",
    "validation_df = final_merge[\n",
    "    (final_merge['estimated_arrival'] > start_date) & \n",
    "    (final_merge['estimated_arrival'] <= end_date)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a UTC-aware timestamp for comparison\n",
    "comparison_date_1 = pd.to_datetime('2019-02-07', utc=True)\n",
    "\n",
    "# Filter the DataFrame to get the test set\n",
    "test_df = final_merge[final_merge['estimated_arrival'] > comparison_date_1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (7296, 24)\n"
     ]
    }
   ],
   "source": [
    "# Ensure that only available columns are used to avoid KeyError\n",
    "available_cts_cols = [col for col in cts_cols if col in train_df.columns]\n",
    "available_cat_cols = [col for col in cat_cols if col in train_df.columns]\n",
    "\n",
    "# Combine continuous and categorical columns that are present\n",
    "selected_columns = available_cts_cols + available_cat_cols\n",
    "\n",
    "# Ensure there are columns to use\n",
    "if not selected_columns:\n",
    "    print(\"No valid columns found for training.\")\n",
    "else:\n",
    "    # Select the available columns from the DataFrame\n",
    "    X_train = train_df[selected_columns]\n",
    "    y_train = train_df['delay']  # Assuming 'delay' is the target variable\n",
    "\n",
    "    # Optional: Print the shape to confirm selection\n",
    "    print(f\"X_train shape: {X_train.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=train_df['delay']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_valid shape: (1949, 24)\n",
      "y_valid shape: (1949,)\n"
     ]
    }
   ],
   "source": [
    "# Ensure only the columns that are present in validation_df are selected\n",
    "available_cts_cols = [col for col in cts_cols if col in validation_df.columns]\n",
    "available_cat_cols = [col for col in cat_cols if col in validation_df.columns]\n",
    "\n",
    "# Combine available continuous and categorical columns\n",
    "selected_columns = available_cts_cols + available_cat_cols\n",
    "\n",
    "# Check if any valid columns are available\n",
    "if not selected_columns:\n",
    "    raise ValueError(\"No valid columns found in validation_df for feature selection.\")\n",
    "\n",
    "# Select the columns from validation_df\n",
    "X_valid = validation_df[selected_columns]\n",
    "\n",
    "# Select the target variable (assuming 'delay' is the target)\n",
    "y_valid = validation_df['delay']\n",
    "\n",
    "# Print the shape to confirm\n",
    "print(f\"X_valid shape: {X_valid.shape}\")\n",
    "print(f\"y_valid shape: {y_valid.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid = validation_df['delay']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test shape: (1227, 24)\n",
      "y_test shape: (1227,)\n"
     ]
    }
   ],
   "source": [
    "# Ensure only available columns are selected from test_df\n",
    "available_cts_cols = [col for col in cts_cols if col in test_df.columns]\n",
    "available_cat_cols = [col for col in cat_cols if col in test_df.columns]\n",
    "\n",
    "# Combine available continuous and categorical columns\n",
    "selected_columns = available_cts_cols + available_cat_cols\n",
    "\n",
    "# Check if any valid columns are available\n",
    "if not selected_columns:\n",
    "    raise ValueError(\"No valid columns found in test_df for feature selection.\")\n",
    "\n",
    "# Select the available columns for X_test\n",
    "X_test = test_df[selected_columns]\n",
    "\n",
    "# Select the target variable (assuming 'delay' is the target)\n",
    "y_test = test_df['delay']\n",
    "\n",
    "# Optional: Print the shape to confirm\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test=test_df['delay']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define the columns to be one-hot encoded\n",
    "encode_columns = ['route_description', 'origin_description', 'destination_description', 'fuel_type', 'gender', 'driving_style']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Initialize the OneHotEncoder with updated parameter\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;, sparse_output=False)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;OneHotEncoder<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.preprocessing.OneHotEncoder.html\">?<span>Documentation for OneHotEncoder</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;, sparse_output=False)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "OneHotEncoder(handle_unknown='ignore', sparse_output=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 3: Fit the encoder on the training data\n",
    "encoder.fit(X_train[encode_columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating names for the new one-hot encoded features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_features = list(encoder.get_feature_names_out(encode_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Transforming the training, validation, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-30 10:56:45,839 WARNING: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "2024-10-30 10:56:45,841 WARNING: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "2024-10-30 10:56:45,843 WARNING: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "2024-10-30 10:56:45,845 WARNING: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "2024-10-30 10:56:45,849 WARNING: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "2024-10-30 10:56:45,851 WARNING: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "2024-10-30 10:56:45,853 WARNING: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "2024-10-30 10:56:45,855 WARNING: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "2024-10-30 10:56:45,857 WARNING: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "2024-10-30 10:56:45,859 WARNING: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "2024-10-30 10:56:45,862 WARNING: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "2024-10-30 10:56:45,864 WARNING: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "2024-10-30 10:56:45,866 WARNING: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "2024-10-30 10:56:45,868 WARNING: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "2024-10-30 10:56:45,870 WARNING: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "2024-10-30 10:56:45,872 WARNING: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "2024-10-30 10:56:45,873 WARNING: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-30 10:56:45,875 WARNING: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "2024-10-30 10:56:45,876 WARNING: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "2024-10-30 10:56:45,878 WARNING: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "2024-10-30 10:56:45,880 WARNING: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "2024-10-30 10:56:45,883 WARNING: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "2024-10-30 10:56:45,885 WARNING: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "2024-10-30 10:56:45,887 WARNING: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "2024-10-30 10:56:45,889 WARNING: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "2024-10-30 10:56:45,914 WARNING: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "2024-10-30 10:56:45,917 WARNING: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "2024-10-30 10:56:45,919 WARNING: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "2024-10-30 10:56:45,920 WARNING: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "2024-10-30 10:56:45,921 WARNING: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "2024-10-30 10:56:45,924 WARNING: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "2024-10-30 10:56:45,926 WARNING: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "2024-10-30 10:56:45,928 WARNING: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "2024-10-30 10:56:45,930 WARNING: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "2024-10-30 10:56:45,933 WARNING: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "2024-10-30 10:56:45,935 WARNING: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "2024-10-30 10:56:45,937 WARNING: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "2024-10-30 10:56:45,939 WARNING: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "2024-10-30 10:56:45,941 WARNING: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "2024-10-30 10:56:45,944 WARNING: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "2024-10-30 10:56:45,946 WARNING: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "2024-10-30 10:56:45,948 WARNING: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train[encoded_features] = encoder.transform(X_train[encode_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-30 10:56:45,970 WARNING: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "2024-10-30 10:56:45,973 WARNING: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "2024-10-30 10:56:45,974 WARNING: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "2024-10-30 10:56:45,976 WARNING: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "2024-10-30 10:56:45,980 WARNING: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "2024-10-30 10:56:45,981 WARNING: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "2024-10-30 10:56:45,983 WARNING: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "2024-10-30 10:56:45,985 WARNING: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "2024-10-30 10:56:45,986 WARNING: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "2024-10-30 10:56:45,989 WARNING: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "2024-10-30 10:56:45,992 WARNING: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "2024-10-30 10:56:45,994 WARNING: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "2024-10-30 10:56:45,996 WARNING: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "2024-10-30 10:56:45,997 WARNING: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "2024-10-30 10:56:45,998 WARNING: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "2024-10-30 10:56:46,000 WARNING: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "2024-10-30 10:56:46,001 WARNING: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "2024-10-30 10:56:46,002 WARNING: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "2024-10-30 10:56:46,006 WARNING: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "2024-10-30 10:56:46,008 WARNING: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "2024-10-30 10:56:46,010 WARNING: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "2024-10-30 10:56:46,013 WARNING: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "2024-10-30 10:56:46,014 WARNING: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "2024-10-30 10:56:46,016 WARNING: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "2024-10-30 10:56:46,019 WARNING: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "2024-10-30 10:56:46,037 WARNING: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "2024-10-30 10:56:46,039 WARNING: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "2024-10-30 10:56:46,041 WARNING: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "2024-10-30 10:56:46,042 WARNING: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "2024-10-30 10:56:46,045 WARNING: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "2024-10-30 10:56:46,046 WARNING: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "2024-10-30 10:56:46,048 WARNING: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "2024-10-30 10:56:46,051 WARNING: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "2024-10-30 10:56:46,052 WARNING: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "2024-10-30 10:56:46,055 WARNING: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "2024-10-30 10:56:46,056 WARNING: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "2024-10-30 10:56:46,059 WARNING: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "2024-10-30 10:56:46,067 WARNING: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "2024-10-30 10:56:46,072 WARNING: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "2024-10-30 10:56:46,074 WARNING: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "2024-10-30 10:56:46,082 WARNING: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "2024-10-30 10:56:46,085 WARNING: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_valid[encoded_features] = encoder.transform(X_valid[encode_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-30 10:56:46,122 WARNING: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "2024-10-30 10:56:46,124 WARNING: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "2024-10-30 10:56:46,131 WARNING: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "2024-10-30 10:56:46,134 WARNING: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "2024-10-30 10:56:46,137 WARNING: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "2024-10-30 10:56:46,139 WARNING: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "2024-10-30 10:56:46,142 WARNING: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "2024-10-30 10:56:46,144 WARNING: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "2024-10-30 10:56:46,146 WARNING: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "2024-10-30 10:56:46,148 WARNING: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "2024-10-30 10:56:46,149 WARNING: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "2024-10-30 10:56:46,152 WARNING: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "2024-10-30 10:56:46,154 WARNING: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "2024-10-30 10:56:46,159 WARNING: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "2024-10-30 10:56:46,161 WARNING: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "2024-10-30 10:56:46,163 WARNING: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "2024-10-30 10:56:46,166 WARNING: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "2024-10-30 10:56:46,170 WARNING: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "2024-10-30 10:56:46,171 WARNING: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "2024-10-30 10:56:46,173 WARNING: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "2024-10-30 10:56:46,176 WARNING: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "2024-10-30 10:56:46,179 WARNING: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "2024-10-30 10:56:46,181 WARNING: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "2024-10-30 10:56:46,184 WARNING: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "2024-10-30 10:56:46,185 WARNING: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "2024-10-30 10:56:46,205 WARNING: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "2024-10-30 10:56:46,207 WARNING: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "2024-10-30 10:56:46,210 WARNING: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "2024-10-30 10:56:46,211 WARNING: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "2024-10-30 10:56:46,215 WARNING: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "2024-10-30 10:56:46,217 WARNING: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "2024-10-30 10:56:46,218 WARNING: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "2024-10-30 10:56:46,221 WARNING: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "2024-10-30 10:56:46,223 WARNING: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "2024-10-30 10:56:46,226 WARNING: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "2024-10-30 10:56:46,227 WARNING: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "2024-10-30 10:56:46,229 WARNING: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "2024-10-30 10:56:46,231 WARNING: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "2024-10-30 10:56:46,234 WARNING: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "2024-10-30 10:56:46,238 WARNING: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "2024-10-30 10:56:46,240 WARNING: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "2024-10-30 10:56:46,243 WARNING: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_test[encoded_features] = encoder.transform(X_test[encode_columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dropping the original categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop(encode_columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid = X_valid.drop(encode_columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.drop(encode_columns, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape after scaling: (7296, 132)\n",
      "X_valid shape after scaling: (1949, 132)\n",
      "X_test shape after scaling: (1227, 132)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Step 1: Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Step 2: Identify numerical columns to scale\n",
    "# Ensure we only scale the encoded categorical columns and other numerical columns if needed.\n",
    "columns_to_scale = X_train.columns  # Adjust if you only want specific columns\n",
    "\n",
    "# Step 3: Fit the scaler on X_train and transform X_train, X_valid, and X_test\n",
    "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train[columns_to_scale]), \n",
    "                              columns=columns_to_scale, index=X_train.index)\n",
    "\n",
    "X_valid_scaled = pd.DataFrame(scaler.transform(X_valid[columns_to_scale]), \n",
    "                              columns=columns_to_scale, index=X_valid.index)\n",
    "\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test[columns_to_scale]), \n",
    "                             columns=columns_to_scale, index=X_test.index)\n",
    "\n",
    "# Optional: Verify the shapes\n",
    "print(f\"X_train shape after scaling: {X_train_scaled.shape}\")\n",
    "print(f\"X_valid shape after scaling: {X_valid_scaled.shape}\")\n",
    "print(f\"X_test shape after scaling: {X_test_scaled.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mlflow in c:\\truck_delay_classification\\.venv\\lib\\site-packages (2.17.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\truck_delay_classification\\.venv\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: xgboost in c:\\truck_delay_classification\\.venv\\lib\\site-packages (2.1.1)\n",
      "Requirement already satisfied: mlflow-skinny==2.17.0 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from mlflow) (2.17.0)\n",
      "Requirement already satisfied: Flask<4 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from mlflow) (3.0.3)\n",
      "Requirement already satisfied: alembic!=1.10.0,<2 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from mlflow) (1.13.3)\n",
      "Requirement already satisfied: docker<8,>=4.0.0 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from mlflow) (7.1.0)\n",
      "Requirement already satisfied: graphene<4 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from mlflow) (3.3)\n",
      "Requirement already satisfied: markdown<4,>=3.3 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from mlflow) (3.7)\n",
      "Requirement already satisfied: matplotlib<4 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from mlflow) (3.9.2)\n",
      "Requirement already satisfied: numpy<3 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from mlflow) (1.26.4)\n",
      "Requirement already satisfied: pandas<3 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from mlflow) (2.1.4)\n",
      "Requirement already satisfied: pyarrow<18,>=4.0.0 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from mlflow) (17.0.0)\n",
      "Requirement already satisfied: scipy<2 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from mlflow) (1.14.1)\n",
      "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from mlflow) (1.4.48)\n",
      "Requirement already satisfied: Jinja2<4,>=3.0 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from mlflow) (3.1.4)\n",
      "Requirement already satisfied: waitress<4 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from mlflow) (3.0.0)\n",
      "Requirement already satisfied: cachetools<6,>=5.0.0 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from mlflow-skinny==2.17.0->mlflow) (5.5.0)\n",
      "Requirement already satisfied: click<9,>=7.0 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from mlflow-skinny==2.17.0->mlflow) (8.1.7)\n",
      "Requirement already satisfied: cloudpickle<4 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from mlflow-skinny==2.17.0->mlflow) (3.1.0)\n",
      "Requirement already satisfied: databricks-sdk<1,>=0.20.0 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from mlflow-skinny==2.17.0->mlflow) (0.34.0)\n",
      "Requirement already satisfied: gitpython<4,>=3.1.9 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from mlflow-skinny==2.17.0->mlflow) (3.1.43)\n",
      "Requirement already satisfied: importlib-metadata!=4.7.0,<9,>=3.7.0 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from mlflow-skinny==2.17.0->mlflow) (8.4.0)\n",
      "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from mlflow-skinny==2.17.0->mlflow) (1.27.0)\n",
      "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from mlflow-skinny==2.17.0->mlflow) (1.27.0)\n",
      "Requirement already satisfied: packaging<25 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from mlflow-skinny==2.17.0->mlflow) (24.1)\n",
      "Requirement already satisfied: protobuf<6,>=3.12.0 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from mlflow-skinny==2.17.0->mlflow) (5.28.2)\n",
      "Requirement already satisfied: pyyaml<7,>=5.1 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from mlflow-skinny==2.17.0->mlflow) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.17.3 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from mlflow-skinny==2.17.0->mlflow) (2.32.3)\n",
      "Requirement already satisfied: sqlparse<1,>=0.4.0 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from mlflow-skinny==2.17.0->mlflow) (0.5.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: Mako in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from alembic!=1.10.0,<2->mlflow) (1.3.5)\n",
      "Requirement already satisfied: typing-extensions>=4 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from alembic!=1.10.0,<2->mlflow) (4.12.2)\n",
      "Requirement already satisfied: pywin32>=304 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from docker<8,>=4.0.0->mlflow) (306)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from docker<8,>=4.0.0->mlflow) (2.2.3)\n",
      "Requirement already satisfied: Werkzeug>=3.0.0 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from Flask<4->mlflow) (3.0.4)\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from Flask<4->mlflow) (2.2.0)\n",
      "Requirement already satisfied: blinker>=1.6.2 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from Flask<4->mlflow) (1.8.2)\n",
      "Requirement already satisfied: graphql-core<3.3,>=3.1 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from graphene<4->mlflow) (3.2.5)\n",
      "Requirement already satisfied: graphql-relay<3.3,>=3.1 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from graphene<4->mlflow) (3.2.0)\n",
      "Requirement already satisfied: aniso8601<10,>=8 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from graphene<4->mlflow) (9.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from Jinja2<4,>=3.0->mlflow) (2.1.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from matplotlib<4->mlflow) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from matplotlib<4->mlflow) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from matplotlib<4->mlflow) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from matplotlib<4->mlflow) (1.4.7)\n",
      "Requirement already satisfied: pillow>=8 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from matplotlib<4->mlflow) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from matplotlib<4->mlflow) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from matplotlib<4->mlflow) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from pandas<3->mlflow) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from pandas<3->mlflow) (2024.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.1.0)\n",
      "Requirement already satisfied: colorama in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from click<9,>=7.0->mlflow-skinny==2.17.0->mlflow) (0.4.6)\n",
      "Requirement already satisfied: google-auth~=2.0 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==2.17.0->mlflow) (2.35.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from gitpython<4,>=3.1.9->mlflow-skinny==2.17.0->mlflow) (4.0.11)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from importlib-metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==2.17.0->mlflow) (3.20.2)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.17.0->mlflow) (1.2.14)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.48b0 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==2.17.0->mlflow) (0.48b0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib<4->mlflow) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from requests<3,>=2.17.3->mlflow-skinny==2.17.0->mlflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from requests<3,>=2.17.3->mlflow-skinny==2.17.0->mlflow) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from requests<3,>=2.17.3->mlflow-skinny==2.17.0->mlflow) (2024.8.30)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.17.0->mlflow) (1.16.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==2.17.0->mlflow) (5.0.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.17.0->mlflow) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.17.0->mlflow) (4.9)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.17.0->mlflow) (0.6.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install mlflow scikit-learn xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['route_avg_temp', 'route_avg_wind_speed', 'route_avg_precip',\n",
       "       'route_avg_humidity', 'route_avg_visibility', 'route_avg_pressure',\n",
       "       'distance', 'average_hours', 'avg_no_of_vehicles', 'truck_age',\n",
       "       ...\n",
       "       'destination_description_Patchy snow possible',\n",
       "       'destination_description_Sunny',\n",
       "       'destination_description_Thundery outbreaks possible',\n",
       "       'destination_description_Torrential rain shower', 'fuel_type_diesel',\n",
       "       'fuel_type_gas', 'gender_female', 'gender_male',\n",
       "       'driving_style_conservative', 'driving_style_proactive'],\n",
       "      dtype='object', length=132)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "2024-10-30 10:56:59,714 WARNING: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/30 10:57:06 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for Logistic Regression: {'C': 0.1, 'max_iter': 100, 'solver': 'lbfgs'}\n",
      "Accuracy: 0.7493150684931507, F1 Score: 0.7228845969625737\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.92      0.84      1020\n",
      "           1       0.66      0.35      0.46       440\n",
      "\n",
      "    accuracy                           0.75      1460\n",
      "   macro avg       0.71      0.64      0.65      1460\n",
      "weighted avg       0.73      0.75      0.72      1460\n",
      "\n",
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n",
      "2024-10-30 10:57:23,050 WARNING: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/30 10:57:27 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for Random Forest: {'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "Accuracy: 0.7808219178082192, F1 Score: 0.7618291754685148\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.93      0.86      1020\n",
      "           1       0.73      0.43      0.54       440\n",
      "\n",
      "    accuracy                           0.78      1460\n",
      "   macro avg       0.76      0.68      0.70      1460\n",
      "weighted avg       0.77      0.78      0.76      1460\n",
      "\n",
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n",
      "2024-10-30 10:57:36,392 WARNING: UserWarning: [10:57:36] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "\n",
      "2024-10-30 10:57:36,576 WARNING: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/30 10:57:40 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for XGBoost: {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 50}\n",
      "Accuracy: 0.7849315068493151, F1 Score: 0.765700426854712\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.94      0.86      1020\n",
      "           1       0.74      0.44      0.55       440\n",
      "\n",
      "    accuracy                           0.78      1460\n",
      "   macro avg       0.77      0.69      0.70      1460\n",
      "weighted avg       0.78      0.78      0.77      1460\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Validation Dataset\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "\n",
    "# Define X_train, X_valid, y_train, y_valid (use your existing datasets)\n",
    "X = X_train_scaled  # Assuming scaled features\n",
    "y = y_train\n",
    "\n",
    "# Split the data further for GridSearch if needed\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Initialize MLflow\n",
    "mlflow.set_experiment(\"ML Models with Hyperparameter Tuning\")\n",
    "\n",
    "# Define a function to train and log models with MLflow\n",
    "def train_and_evaluate_model(model, param_grid, model_name):\n",
    "    with mlflow.start_run(run_name=model_name):\n",
    "        # GridSearchCV for hyperparameter tuning\n",
    "        grid_search = GridSearchCV(model, param_grid, cv=3, n_jobs=-1, verbose=2)\n",
    "        grid_search.fit(X_train_split, y_train_split)\n",
    "\n",
    "        # Get the best model from GridSearch\n",
    "        best_model = grid_search.best_estimator_\n",
    "\n",
    "        # Predict on validation data\n",
    "        y_pred = best_model.predict(X_val_split)\n",
    "\n",
    "        # Evaluate performance\n",
    "        acc = accuracy_score(y_val_split, y_pred)\n",
    "        f1 = f1_score(y_val_split, y_pred, average='weighted')\n",
    "\n",
    "        # Log parameters, metrics, and model\n",
    "        mlflow.log_params(grid_search.best_params_)\n",
    "        mlflow.log_metrics({\"accuracy\": acc, \"f1_score\": f1})\n",
    "        mlflow.sklearn.log_model(best_model, model_name)\n",
    "\n",
    "        # Print the results\n",
    "        print(f\"Best Parameters for {model_name}: {grid_search.best_params_}\")\n",
    "        print(f\"Accuracy: {acc}, F1 Score: {f1}\")\n",
    "        print(classification_report(y_val_split, y_pred))\n",
    "\n",
    "# Hyperparameter grids for each model\n",
    "logreg_params = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'solver': ['lbfgs', 'liblinear'],\n",
    "    'max_iter': [100, 200]\n",
    "}\n",
    "\n",
    "rf_params = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "xgb_params = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 6, 10]\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "train_and_evaluate_model(LogisticRegression(), logreg_params, \"Logistic Regression\")\n",
    "train_and_evaluate_model(RandomForestClassifier(), rf_params, \"Random Forest\")\n",
    "train_and_evaluate_model(XGBClassifier(use_label_encoder=False, eval_metric='logloss'), xgb_params, \"XGBoost\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Accuracy for the validation dataset\n",
    "# from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "\n",
    "# # Function to train and evaluate the model, including validation accuracy\n",
    "# def train_and_evaluate_model(model, param_grid, model_name):\n",
    "#     with mlflow.start_run(run_name=model_name):\n",
    "#         # GridSearchCV for hyperparameter tuning\n",
    "#         grid_search = GridSearchCV(model, param_grid, cv=3, n_jobs=-1, verbose=2)\n",
    "#         grid_search.fit(X_train_split, y_train_split)\n",
    "\n",
    "#         # Get the best model from GridSearch\n",
    "#         best_model = grid_search.best_estimator_\n",
    "\n",
    "#         # Predict on validation data\n",
    "#         y_val_pred = best_model.predict(X_val_split)\n",
    "\n",
    "#         # Calculate validation accuracy and F1 score\n",
    "#         val_accuracy = accuracy_score(y_val_split, y_val_pred)\n",
    "#         val_f1 = f1_score(y_val_split, y_val_pred, average='weighted')\n",
    "\n",
    "#         # Log parameters, metrics, and model with MLflow\n",
    "#         mlflow.log_params(grid_search.best_params_)\n",
    "#         mlflow.log_metrics({\"val_accuracy\": val_accuracy, \"val_f1_score\": val_f1})\n",
    "#         mlflow.sklearn.log_model(best_model, model_name)\n",
    "\n",
    "#         # Print the evaluation results\n",
    "#         print(f\"Best Parameters for {model_name}: {grid_search.best_params_}\")\n",
    "#         print(f\"Validation Accuracy: {val_accuracy:.4f}, F1 Score: {val_f1:.4f}\")\n",
    "#         print(classification_report(y_val_split, y_val_pred))\n",
    "\n",
    "# # Example hyperparameter grids\n",
    "# logreg_params = {'C': [0.1, 1, 10], 'solver': ['lbfgs', 'liblinear'], 'max_iter': [100, 200]}\n",
    "# rf_params = {'n_estimators': [50, 100], 'max_depth': [None, 10], 'min_samples_split': [2, 5]}\n",
    "# xgb_params = {'n_estimators': [50, 100], 'learning_rate': [0.01, 0.1], 'max_depth': [3, 6]}\n",
    "\n",
    "# # Train models with the new function\n",
    "# train_and_evaluate_model(LogisticRegression(), logreg_params, \"Logistic Regression\")\n",
    "# train_and_evaluate_model(RandomForestClassifier(), rf_params, \"Random Forest\")\n",
    "# train_and_evaluate_model(XGBClassifier(use_label_encoder=False, eval_metric='logloss'), xgb_params, \"XGBoost\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Training dataset\n",
    "# import mlflow\n",
    "# import mlflow.sklearn\n",
    "# from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from xgboost import XGBClassifier\n",
    "# from sklearn.metrics import classification_report, accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "# # Define X_train, y_train (use your existing datasets)\n",
    "# X_train_split = X_train_scaled  # Assuming scaled features\n",
    "# y_train_split = y_train  # Assuming labels\n",
    "\n",
    "# # Initialize MLflow\n",
    "# mlflow.set_experiment(\"ML Models with Hyperparameter Tuning\")\n",
    "\n",
    "# # Define a function to train and log models with MLflow\n",
    "# def train_and_evaluate_on_train(model, param_grid, model_name):\n",
    "#     with mlflow.start_run(run_name=model_name):\n",
    "#         # GridSearchCV for hyperparameter tuning\n",
    "#         grid_search = GridSearchCV(model, param_grid, cv=3, n_jobs=-1, verbose=2)\n",
    "#         grid_search.fit(X_train_split, y_train_split)\n",
    "\n",
    "#         # Get the best model from GridSearch\n",
    "#         best_model = grid_search.best_estimator_\n",
    "\n",
    "#         # Evaluate on the training dataset\n",
    "#         y_train_pred = best_model.predict(X_train_split)\n",
    "#         train_acc = accuracy_score(y_train_split, y_train_pred)\n",
    "#         train_f1 = f1_score(y_train_split, y_train_pred, average='weighted')\n",
    "#         train_precision = precision_score(y_train_split, y_train_pred, average='weighted')\n",
    "#         train_recall = recall_score(y_train_split, y_train_pred, average='weighted')\n",
    "\n",
    "#         # Log parameters, metrics, and model\n",
    "#         mlflow.log_params(grid_search.best_params_)\n",
    "#         mlflow.log_metrics({\"train_accuracy\": train_acc, \"train_f1_score\": train_f1,\n",
    "#                             \"train_precision\": train_precision, \"train_recall\": train_recall})\n",
    "#         mlflow.sklearn.log_model(best_model, model_name)\n",
    "\n",
    "#         # Print the evaluation results for training\n",
    "#         print(f\"Best Parameters for {model_name}: {grid_search.best_params_}\")\n",
    "#         print(f\"Training Accuracy: {train_acc:.4f}, F1 Score: {train_f1:.4f}, Precision: {train_precision:.4f}, Recall: {train_recall:.4f}\")\n",
    "#         print(\"Training classification report:\")\n",
    "#         print(classification_report(y_train_split, y_train_pred))\n",
    "\n",
    "# # Hyperparameter grids for each model\n",
    "# logreg_params = {\n",
    "#     'C': [0.1, 1, 10],\n",
    "#     'solver': ['lbfgs', 'liblinear'],\n",
    "#     'max_iter': [100, 200]\n",
    "# }\n",
    "\n",
    "# rf_params = {\n",
    "#     'n_estimators': [50, 175, 150],\n",
    "#     'max_depth': [None, 10, 15],\n",
    "#     'min_samples_split': [2, 3],\n",
    "#     'min_samples_leaf': [1, 3]\n",
    "# }\n",
    "\n",
    "# xgb_params = {\n",
    "#     'n_estimators': [50, 100, 200],\n",
    "#     'learning_rate': [0.01, 0.1, 0.2],\n",
    "#     'max_depth': [3, 6, 10]\n",
    "# }\n",
    "\n",
    "# # Train and evaluate models on training data only\n",
    "# train_and_evaluate_on_train(LogisticRegression(), logreg_params, \"Logistic Regression\")\n",
    "# train_and_evaluate_on_train(RandomForestClassifier(), rf_params, \"Random Forest\")\n",
    "# train_and_evaluate_on_train(XGBClassifier(use_label_encoder=False, eval_metric='logloss'), xgb_params, \"XGBoost\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Testing dataset\n",
    "# import mlflow\n",
    "# import mlflow.sklearn\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from xgboost import XGBClassifier\n",
    "# from sklearn.metrics import classification_report, accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "# # Define X_test and y_test (use your existing testing datasets)\n",
    "# X_test_split = X_test_scaled  # Assuming scaled test features\n",
    "# y_test_split = y_test  # Assuming test labels\n",
    "\n",
    "# # Initialize MLflow\n",
    "# mlflow.set_experiment(\"ML Models with Hyperparameter Tuning\")\n",
    "\n",
    "# # Define a function to evaluate models on testing data with MLflow\n",
    "# def evaluate_on_test(model, param_grid, model_name):\n",
    "#     with mlflow.start_run(run_name=model_name):\n",
    "#         # GridSearchCV for hyperparameter tuning\n",
    "#         grid_search = GridSearchCV(model, param_grid, cv=3, n_jobs=-1, verbose=2)\n",
    "#         grid_search.fit(X_train_split, y_train_split)  # Train on the training set\n",
    "\n",
    "#         # Get the best model from GridSearch\n",
    "#         best_model = grid_search.best_estimator_\n",
    "\n",
    "#         # Evaluate on the testing dataset\n",
    "#         y_test_pred = best_model.predict(X_test_split)\n",
    "#         test_acc = accuracy_score(y_test_split, y_test_pred)\n",
    "#         test_f1 = f1_score(y_test_split, y_test_pred, average='weighted')\n",
    "#         test_precision = precision_score(y_test_split, y_test_pred, average='weighted')\n",
    "#         test_recall = recall_score(y_test_split, y_test_pred, average='weighted')\n",
    "\n",
    "#         # Log parameters, metrics, and model\n",
    "#         mlflow.log_params(grid_search.best_params_)\n",
    "#         mlflow.log_metrics({\"test_accuracy\": test_acc, \"test_f1_score\": test_f1,\n",
    "#                             \"test_precision\": test_precision, \"test_recall\": test_recall})\n",
    "#         mlflow.sklearn.log_model(best_model, model_name)\n",
    "\n",
    "#         # Print the evaluation results for testing\n",
    "#         print(f\"Best Parameters for {model_name}: {grid_search.best_params_}\")\n",
    "#         print(f\"Test Accuracy: {test_acc:.4f}, F1 Score: {test_f1:.4f}, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}\")\n",
    "#         print(\"Test classification report:\")\n",
    "#         print(classification_report(y_test_split, y_test_pred))\n",
    "\n",
    "# # Hyperparameter grids for each model\n",
    "# logreg_params = {\n",
    "#     'C': [0.1, 1, 10],\n",
    "#     'solver': ['lbfgs', 'liblinear'],\n",
    "#     'max_iter': [100, 200]\n",
    "# }\n",
    "\n",
    "# rf_params = {\n",
    "#     'n_estimators': [50, 150, 200],\n",
    "#     'max_depth': [None, 10, 20],\n",
    "#     'min_samples_split': [2, 5],\n",
    "#     'min_samples_leaf': [1, 2]\n",
    "# }\n",
    "\n",
    "# xgb_params = {\n",
    "#     'n_estimators': [50, 100, 200],\n",
    "#     'learning_rate': [0.01, 0.1, 0.2],\n",
    "#     'max_depth': [3, 6, 10]\n",
    "# }\n",
    "\n",
    "# # Train models on training set and evaluate on testing data\n",
    "# evaluate_on_test(LogisticRegression(), logreg_params, \"Logistic Regression\")\n",
    "# evaluate_on_test(RandomForestClassifier(), rf_params, \"Random Forest\")\n",
    "# evaluate_on_test(XGBClassifier(use_label_encoder=False, eval_metric='logloss'), xgb_params, \"XGBoost\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Training data\n",
    "# import mlflow\n",
    "# import mlflow.sklearn\n",
    "# import pickle\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from xgboost import XGBClassifier\n",
    "# from sklearn.metrics import classification_report, accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "# # Define X_train, y_train (use your existing datasets)\n",
    "# X_train_split = X_train_scaled  # Assuming scaled features\n",
    "# y_train_split = y_train  # Assuming labels\n",
    "\n",
    "# # Initialize MLflow\n",
    "# mlflow.set_experiment(\"ML Models with Hyperparameter Tuning\")\n",
    "\n",
    "# # Define a function to train, evaluate, and save models as pickle files\n",
    "# def train_evaluate_save_as_pickle(model, param_grid, model_name):\n",
    "#     with mlflow.start_run(run_name=model_name):\n",
    "#         # GridSearchCV for hyperparameter tuning\n",
    "#         grid_search = GridSearchCV(model, param_grid, cv=3, n_jobs=-1, verbose=2)\n",
    "#         grid_search.fit(X_train_split, y_train_split)\n",
    "\n",
    "#         # Get the best model from GridSearch\n",
    "#         best_model = grid_search.best_estimator_\n",
    "\n",
    "#         # Evaluate on the training dataset\n",
    "#         y_train_pred = best_model.predict(X_train_split)\n",
    "#         train_acc = accuracy_score(y_train_split, y_train_pred)\n",
    "#         train_f1 = f1_score(y_train_split, y_train_pred, average='weighted')\n",
    "#         train_precision = precision_score(y_train_split, y_train_pred, average='weighted')\n",
    "#         train_recall = recall_score(y_train_split, y_train_pred, average='weighted')\n",
    "\n",
    "#         # Log parameters, metrics, and model with MLflow\n",
    "#         mlflow.log_params(grid_search.best_params_)\n",
    "#         mlflow.log_metrics({\"train_accuracy\": train_acc, \"train_f1_score\": train_f1,\n",
    "#                             \"train_precision\": train_precision, \"train_recall\": train_recall})\n",
    "#         mlflow.sklearn.log_model(best_model, model_name)\n",
    "\n",
    "#         # Save model as a pickle file\n",
    "#         with open(f\"./models/{model_name}_model.pkl\", 'wb') as f:\n",
    "#             pickle.dump(best_model, f)\n",
    "#         print(f\"Model saved as {model_name}_model.pkl\")\n",
    "\n",
    "#         # Print the evaluation results for training\n",
    "#         print(f\"Best Parameters for {model_name}: {grid_search.best_params_}\")\n",
    "#         print(f\"Training Accuracy: {train_acc:.4f}, F1 Score: {train_f1:.4f}, Precision: {train_precision:.4f}, Recall: {train_recall:.4f}\")\n",
    "#         print(\"Training classification report:\")\n",
    "#         print(classification_report(y_train_split, y_train_pred))\n",
    "\n",
    "# # Hyperparameter grids for each model\n",
    "# logreg_params = {\n",
    "#     'C': [0.1, 1, 10],\n",
    "#     'solver': ['lbfgs', 'liblinear'],\n",
    "#     'max_iter': [100, 200]\n",
    "# }\n",
    "\n",
    "# rf_params = {\n",
    "#     'n_estimators': [50, 175, 150],\n",
    "#     'max_depth': [None, 10, 15],\n",
    "#     'min_samples_split': [2, 3],\n",
    "#     'min_samples_leaf': [1, 3]\n",
    "# }\n",
    "\n",
    "# xgb_params = {\n",
    "#     'n_estimators': [50, 100, 200],\n",
    "#     'learning_rate': [0.01, 0.1, 0.2],\n",
    "#     'max_depth': [3, 6, 10]\n",
    "# }\n",
    "\n",
    "# # Train, evaluate, and save models on training data\n",
    "# train_evaluate_save_as_pickle(LogisticRegression(), logreg_params, \"Logistic_Regression\")\n",
    "# train_evaluate_save_as_pickle(RandomForestClassifier(), rf_params, \"Random_Forest\")\n",
    "# train_evaluate_save_as_pickle(XGBClassifier(use_label_encoder=False, eval_metric='logloss'), xgb_params, \"XGBoost\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hopsworks in c:\\truck_delay_classification\\.venv\\lib\\site-packages (3.7.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: hsfs<3.8.0,>=3.7.0 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from hsfs[python]<3.8.0,>=3.7.0->hopsworks) (3.7.7)\n",
      "Requirement already satisfied: hsml<3.8.0,>=3.7.0 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from hopsworks) (3.7.1)\n",
      "Requirement already satisfied: pyhumps==1.6.1 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from hopsworks) (1.6.1)\n",
      "Requirement already satisfied: requests in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from hopsworks) (2.32.3)\n",
      "Requirement already satisfied: furl in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from hopsworks) (2.1.3)\n",
      "Requirement already satisfied: boto3 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from hopsworks) (1.35.22)\n",
      "Requirement already satisfied: pyjks in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from hopsworks) (20.0.0)\n",
      "Requirement already satisfied: mock in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from hopsworks) (5.1.0)\n",
      "Requirement already satisfied: tqdm in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from hopsworks) (4.66.5)\n",
      "Requirement already satisfied: pandas<2.2.0 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (2.1.4)\n",
      "Requirement already satisfied: numpy<2 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (1.26.4)\n",
      "Requirement already satisfied: avro==1.11.3 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (1.11.3)\n",
      "Requirement already satisfied: sqlalchemy<=1.4.48 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (1.4.48)\n",
      "Requirement already satisfied: PyMySQL[rsa] in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (1.1.1)\n",
      "Requirement already satisfied: great-expectations==0.18.12 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (0.18.12)\n",
      "Requirement already satisfied: tzlocal in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (5.2)\n",
      "Requirement already satisfied: fsspec in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (2024.9.0)\n",
      "Requirement already satisfied: retrying in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (1.3.4)\n",
      "Requirement already satisfied: aiomysql in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (0.2.0)\n",
      "Requirement already satisfied: opensearch-py<=2.4.2,>=1.1.0 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (2.4.2)\n",
      "Requirement already satisfied: altair<5.0.0,>=4.2.1 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (4.2.2)\n",
      "Requirement already satisfied: Click>=7.1.2 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (8.1.7)\n",
      "Requirement already satisfied: colorama>=0.4.3 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (0.4.6)\n",
      "Requirement already satisfied: cryptography>=3.2 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (43.0.1)\n",
      "Requirement already satisfied: Ipython>=7.16.3 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (8.27.0)\n",
      "Requirement already satisfied: ipywidgets>=7.5.1 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (8.1.5)\n",
      "Requirement already satisfied: jinja2>=2.10 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (3.1.4)\n",
      "Requirement already satisfied: jsonpatch>=1.22 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (1.33)\n",
      "Requirement already satisfied: jsonschema>=2.5.1 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (4.23.0)\n",
      "Requirement already satisfied: makefun<2,>=1.7.0 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (1.15.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.7.1 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (3.22.0)\n",
      "Requirement already satisfied: mistune>=0.8.4 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (3.0.2)\n",
      "Requirement already satisfied: nbformat>=5.0 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (5.10.4)\n",
      "Requirement already satisfied: notebook>=6.4.10 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (7.2.2)\n",
      "Requirement already satisfied: packaging in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (24.1)\n",
      "Requirement already satisfied: pydantic>=1.9.2 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (2.9.2)\n",
      "Requirement already satisfied: pyparsing>=2.4 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2021.3 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (2024.2)\n",
      "Requirement already satisfied: ruamel.yaml<0.17.18,>=0.16 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (0.17.17)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (1.14.1)\n",
      "Requirement already satisfied: typing-extensions>=3.10.0.0 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (4.12.2)\n",
      "Requirement already satisfied: urllib3>=1.26 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (2.2.3)\n",
      "Requirement already satisfied: pyhopshive[thrift] in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from hsfs[python]<3.8.0,>=3.7.0->hopsworks) (0.6.4.1.dev0)\n",
      "Requirement already satisfied: pyarrow>=10.0 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from hsfs[python]<3.8.0,>=3.7.0->hopsworks) (17.0.0)\n",
      "Requirement already satisfied: confluent-kafka<=2.3.0 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from hsfs[python]<3.8.0,>=3.7.0->hopsworks) (2.3.0)\n",
      "Requirement already satisfied: fastavro<=1.8.4,>=1.4.11 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from hsfs[python]<3.8.0,>=3.7.0->hopsworks) (1.8.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from requests->hopsworks) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from requests->hopsworks) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from requests->hopsworks) (2024.8.30)\n",
      "Requirement already satisfied: botocore<1.36.0,>=1.35.22 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from boto3->hopsworks) (1.35.22)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from boto3->hopsworks) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from boto3->hopsworks) (0.10.2)\n",
      "Requirement already satisfied: six>=1.8.0 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from furl->hopsworks) (1.16.0)\n",
      "Requirement already satisfied: orderedmultidict>=1.0.1 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from furl->hopsworks) (1.0.1)\n",
      "Requirement already satisfied: javaobj-py3 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from pyjks->hopsworks) (0.4.4)\n",
      "Requirement already satisfied: pyasn1>=0.3.5 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from pyjks->hopsworks) (0.6.1)\n",
      "Requirement already satisfied: pyasn1-modules in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from pyjks->hopsworks) (0.4.1)\n",
      "Requirement already satisfied: pycryptodomex in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from pyjks->hopsworks) (3.20.0)\n",
      "Requirement already satisfied: twofish in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from pyjks->hopsworks) (0.3.0)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from pandas<2.2.0->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (2024.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from sqlalchemy<=1.4.48->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (3.1.0)\n",
      "Requirement already satisfied: future in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from pyhopshive[thrift]; extra == \"python\"->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (1.0.0)\n",
      "Requirement already satisfied: thrift>=0.10.0 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from pyhopshive[thrift]; extra == \"python\"->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (0.20.0)\n",
      "Requirement already satisfied: entrypoints in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from altair<5.0.0,>=4.2.1->great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (0.4)\n",
      "Requirement already satisfied: toolz in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from altair<5.0.0,>=4.2.1->great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (0.12.1)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from cryptography>=3.2->great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (1.17.1)\n",
      "Requirement already satisfied: decorator in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from Ipython>=7.16.3->great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from Ipython>=7.16.3->great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from Ipython>=7.16.3->great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from Ipython>=7.16.3->great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (3.0.47)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from Ipython>=7.16.3->great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (2.18.0)\n",
      "Requirement already satisfied: stack-data in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from Ipython>=7.16.3->great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (0.6.3)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from Ipython>=7.16.3->great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (5.14.3)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from ipywidgets>=7.5.1->great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (0.2.2)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from ipywidgets>=7.5.1->great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from ipywidgets>=7.5.1->great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (3.0.13)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from jinja2>=2.10->great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (2.1.5)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from jsonpatch>=1.22->great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (3.0.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from jsonschema>=2.5.1->great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (24.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from jsonschema>=2.5.1->great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from jsonschema>=2.5.1->great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from jsonschema>=2.5.1->great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (0.20.0)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from nbformat>=5.0->great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (2.20.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from nbformat>=5.0->great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (5.7.2)\n",
      "Requirement already satisfied: jupyter-server<3,>=2.4.0 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from notebook>=6.4.10->great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (2.14.2)\n",
      "Requirement already satisfied: jupyterlab-server<3,>=2.27.1 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from notebook>=6.4.10->great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (2.27.3)\n",
      "Requirement already satisfied: jupyterlab<4.3,>=4.2.0 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from notebook>=6.4.10->great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (4.2.5)\n",
      "Requirement already satisfied: notebook-shim<0.3,>=0.2 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from notebook>=6.4.10->great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (0.2.4)\n",
      "Requirement already satisfied: tornado>=6.2.0 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from notebook>=6.4.10->great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (6.4.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from pydantic>=1.9.2->great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from pydantic>=1.9.2->great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (2.23.4)\n",
      "Requirement already satisfied: pycparser in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from cffi>=1.12->cryptography>=3.2->great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (2.22)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from jedi>=0.16->Ipython>=7.16.3->great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (0.8.4)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat>=5.0->great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (4.3.6)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat>=5.0->great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (306)\n",
      "Requirement already satisfied: anyio>=3.1.0 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=6.4.10->great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (4.5.0)\n",
      "Requirement already satisfied: argon2-cffi>=21.1 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=6.4.10->great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (23.1.0)\n",
      "Requirement already satisfied: jupyter-client>=7.4.4 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=6.4.10->great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (8.6.3)\n",
      "Requirement already satisfied: jupyter-events>=0.9.0 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=6.4.10->great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (0.10.0)\n",
      "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=6.4.10->great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (0.5.3)\n",
      "Requirement already satisfied: nbconvert>=6.4.4 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=6.4.10->great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (7.16.4)\n",
      "Requirement already satisfied: overrides>=5.0 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=6.4.10->great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (7.7.0)\n",
      "Requirement already satisfied: prometheus-client>=0.9 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=6.4.10->great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (0.20.0)\n",
      "Requirement already satisfied: pywinpty>=2.0.1 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=6.4.10->great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (2.0.13)\n",
      "Requirement already satisfied: pyzmq>=24 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=6.4.10->great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (26.2.0)\n",
      "Requirement already satisfied: send2trash>=1.8.2 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=6.4.10->great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (1.8.3)\n",
      "Requirement already satisfied: terminado>=0.8.3 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=6.4.10->great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (0.18.1)\n",
      "Requirement already satisfied: websocket-client>=1.7 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=6.4.10->great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (1.8.0)\n",
      "Requirement already satisfied: async-lru>=1.0.0 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from jupyterlab<4.3,>=4.2.0->notebook>=6.4.10->great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (2.0.4)\n",
      "Requirement already satisfied: httpx>=0.25.0 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from jupyterlab<4.3,>=4.2.0->notebook>=6.4.10->great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (0.27.2)\n",
      "Requirement already satisfied: ipykernel>=6.5.0 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from jupyterlab<4.3,>=4.2.0->notebook>=6.4.10->great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (6.29.5)\n",
      "Requirement already satisfied: jupyter-lsp>=2.0.0 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from jupyterlab<4.3,>=4.2.0->notebook>=6.4.10->great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (2.2.5)\n",
      "Requirement already satisfied: setuptools>=40.1.0 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from jupyterlab<4.3,>=4.2.0->notebook>=6.4.10->great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (75.1.0)\n",
      "Requirement already satisfied: babel>=2.10 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->notebook>=6.4.10->great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (2.16.0)\n",
      "Requirement already satisfied: json5>=0.9.0 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->notebook>=6.4.10->great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (0.9.25)\n",
      "Requirement already satisfied: wcwidth in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->Ipython>=7.16.3->great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from stack-data->Ipython>=7.16.3->great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from stack-data->Ipython>=7.16.3->great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from stack-data->Ipython>=7.16.3->great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (0.2.3)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook>=6.4.10->great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (1.3.1)\n",
      "Requirement already satisfied: argon2-cffi-bindings in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook>=6.4.10->great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (21.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from httpx>=0.25.0->jupyterlab<4.3,>=4.2.0->notebook>=6.4.10->great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from httpcore==1.*->httpx>=0.25.0->jupyterlab<4.3,>=4.2.0->notebook>=6.4.10->great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (0.14.0)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from ipykernel>=6.5.0->jupyterlab<4.3,>=4.2.0->notebook>=6.4.10->great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (1.8.5)\n",
      "Requirement already satisfied: nest-asyncio in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from ipykernel>=6.5.0->jupyterlab<4.3,>=4.2.0->notebook>=6.4.10->great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (1.6.0)\n",
      "Requirement already satisfied: psutil in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from ipykernel>=6.5.0->jupyterlab<4.3,>=4.2.0->notebook>=6.4.10->great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (6.0.0)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=6.4.10->great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (2.0.7)\n",
      "Requirement already satisfied: pyyaml>=5.3 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=6.4.10->great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (6.0.2)\n",
      "Requirement already satisfied: rfc3339-validator in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=6.4.10->great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=6.4.10->great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (0.1.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=6.4.10->great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (4.12.3)\n",
      "Requirement already satisfied: bleach!=5.0.0 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=6.4.10->great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (6.1.0)\n",
      "Requirement already satisfied: defusedxml in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=6.4.10->great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=6.4.10->great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (0.3.0)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=6.4.10->great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (0.10.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=6.4.10->great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (1.5.1)\n",
      "Requirement already satisfied: tinycss2 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=6.4.10->great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (1.3.0)\n",
      "Requirement already satisfied: webencodings in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from bleach!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=6.4.10->great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (0.5.1)\n",
      "Requirement already satisfied: fqdn in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=6.4.10->great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (1.5.1)\n",
      "Requirement already satisfied: isoduration in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=6.4.10->great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (20.11.0)\n",
      "Requirement already satisfied: uri-template in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=6.4.10->great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=24.6.0 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=6.4.10->great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (24.8.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from beautifulsoup4->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=6.4.10->great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (2.6)\n",
      "Requirement already satisfied: arrow>=0.15.0 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=6.4.10->great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (1.3.0)\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in c:\\truck_delay_classification\\.venv\\lib\\site-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=6.4.10->great-expectations==0.18.12->hsfs<3.8.0,>=3.7.0->hsfs[python]<3.8.0,>=3.7.0->hopsworks) (2.9.0.20240906)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install hopsworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All required columns are present.\n"
     ]
    }
   ],
   "source": [
    "required_columns = ['accident', 'is_midnight', 'ratings']\n",
    "missing_columns = [col for col in required_columns if col not in X_train_split.columns]\n",
    "\n",
    "if missing_columns:\n",
    "    print(f\"Missing columns: {missing_columns}\")\n",
    "else:\n",
    "    print(\"All required columns are present.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['route_avg_temp', 'route_avg_wind_speed', 'route_avg_precip',\n",
      "       'route_avg_humidity', 'route_avg_visibility', 'route_avg_pressure',\n",
      "       'distance', 'average_hours', 'avg_no_of_vehicles', 'truck_age',\n",
      "       ...\n",
      "       'destination_description_Patchy snow possible',\n",
      "       'destination_description_Sunny',\n",
      "       'destination_description_Thundery outbreaks possible',\n",
      "       'destination_description_Torrential rain shower', 'fuel_type_diesel',\n",
      "       'fuel_type_gas', 'gender_female', 'gender_male',\n",
      "       'driving_style_conservative', 'driving_style_proactive'],\n",
      "      dtype='object', length=132)\n",
      "delay\n"
     ]
    }
   ],
   "source": [
    "print(X_train_split.columns)\n",
    "print(y_train_split.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "2024-10-30 10:58:49,325 WARNING: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/30 10:58:55 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './models/Logistic_Regression_model.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 81\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m best_model, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m: train_acc, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1_score\u001b[39m\u001b[38;5;124m\"\u001b[39m: train_f1, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m\"\u001b[39m: train_precision, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecall\u001b[39m\u001b[38;5;124m\"\u001b[39m: train_recall}\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# Train models and get the best models and their metrics\u001b[39;00m\n\u001b[1;32m---> 81\u001b[0m best_logistic_model, logistic_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_log_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mLogisticRegression\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogreg_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLogistic_Regression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m best_rf_model, rf_metrics \u001b[38;5;241m=\u001b[39m train_and_log_model(RandomForestClassifier(), rf_params, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRandom_Forest\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     83\u001b[0m best_xgb_model, xgb_metrics \u001b[38;5;241m=\u001b[39m train_and_log_model(XGBClassifier(use_label_encoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, eval_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogloss\u001b[39m\u001b[38;5;124m'\u001b[39m), xgb_params, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXGBoost\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[42], line 68\u001b[0m, in \u001b[0;36mtrain_and_log_model\u001b[1;34m(model, param_grid, model_name)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# Save the model as a pickle file\u001b[39;00m\n\u001b[0;32m     67\u001b[0m pickle_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./models/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_model.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 68\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpickle_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     69\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump(best_model, f)\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel saved as \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_model.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Truck_Delay_Classification\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './models/Logistic_Regression_model.pkl'"
     ]
    }
   ],
   "source": [
    "#Training Data\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import pickle\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, precision_score, recall_score\n",
    "import hopsworks\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Assuming X_train_split and y_train_split are your feature and label datasets\n",
    "# Define X_train_split, y_train_split (use your existing datasets)\n",
    "X_train_split = X_train_scaled  # Assuming scaled features\n",
    "y_train_split = y_train  # Assuming labels\n",
    "\n",
    "# Initialize MLflow experiment\n",
    "mlflow.set_experiment(\"ML Models with Hyperparameter Tuning\")\n",
    "\n",
    "# Hyperparameter grids for each model\n",
    "logreg_params = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'solver': ['lbfgs', 'liblinear'],\n",
    "    'max_iter': [100, 200]\n",
    "}\n",
    "\n",
    "rf_params = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [None, 10, 15],\n",
    "    'min_samples_split': [2, 3],\n",
    "    'min_samples_leaf': [1, 3]\n",
    "}\n",
    "\n",
    "xgb_params = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 6, 10]\n",
    "}\n",
    "\n",
    "# Function to perform model training, evaluation, and saving\n",
    "def train_and_log_model(model, param_grid, model_name):\n",
    "    # Start MLflow run\n",
    "    with mlflow.start_run(run_name=model_name):\n",
    "        # GridSearchCV for hyperparameter tuning\n",
    "        grid_search = GridSearchCV(model, param_grid, cv=3, n_jobs=-1, verbose=2)\n",
    "        grid_search.fit(X_train_split, y_train_split)\n",
    "        \n",
    "        # Get the best model from GridSearchCV\n",
    "        best_model = grid_search.best_estimator_\n",
    "        \n",
    "        # Evaluate model on the training set\n",
    "        y_train_pred = best_model.predict(X_train_split)\n",
    "        train_acc = accuracy_score(y_train_split, y_train_pred)\n",
    "        train_f1 = f1_score(y_train_split, y_train_pred, average='weighted')\n",
    "        train_precision = precision_score(y_train_split, y_train_pred, average='weighted')\n",
    "        train_recall = recall_score(y_train_split, y_train_pred, average='weighted')\n",
    "        \n",
    "        # Log parameters, metrics, and the model with MLflow\n",
    "        mlflow.log_params(grid_search.best_params_)\n",
    "        mlflow.log_metrics({\"train_accuracy\": train_acc, \"train_f1_score\": train_f1,\n",
    "                            \"train_precision\": train_precision, \"train_recall\": train_recall})\n",
    "        mlflow.sklearn.log_model(best_model, model_name)\n",
    "        \n",
    "        # Save the model as a pickle file\n",
    "        pickle_file = f\"./models/{model_name}_model.pkl\"\n",
    "        with open(pickle_file, 'wb') as f:\n",
    "            pickle.dump(best_model, f)\n",
    "        print(f\"Model saved as {model_name}_model.pkl\")\n",
    "        \n",
    "        # Print evaluation results\n",
    "        print(f\"Best Parameters for {model_name}: {grid_search.best_params_}\")\n",
    "        print(f\"Training Accuracy: {train_acc:.4f}, F1 Score: {train_f1:.4f}, Precision: {train_precision:.4f}, Recall: {train_recall:.4f}\")\n",
    "        print(\"Training classification report:\")\n",
    "        print(classification_report(y_train_split, y_train_pred))\n",
    "        \n",
    "        return best_model, {\"accuracy\": train_acc, \"f1_score\": train_f1, \"precision\": train_precision, \"recall\": train_recall}\n",
    "\n",
    "# Train models and get the best models and their metrics\n",
    "best_logistic_model, logistic_metrics = train_and_log_model(LogisticRegression(), logreg_params, \"Logistic_Regression\")\n",
    "best_rf_model, rf_metrics = train_and_log_model(RandomForestClassifier(), rf_params, \"Random_Forest\")\n",
    "best_xgb_model, xgb_metrics = train_and_log_model(XGBClassifier(use_label_encoder=False, eval_metric='logloss'), xgb_params, \"XGBoost\")\n",
    "\n",
    "# Save XGBoost model to Hopsworks\n",
    "# Connect to Hopsworks\n",
    "project = hopsworks.login(api_key_value=\"O4IOxWozstKu0BFQ.07C1tbvgVI5C4XNLbLrGH4PS4t0EqBYN00ex8318TNIkl82WwDi3Vh9MidMrCA83\")\n",
    "\n",
    "# Get the Model Registry\n",
    "mr = project.get_model_registry()\n",
    "\n",
    "# Directory to save the model locally (for later upload to Hopsworks)\n",
    "model_dir = \"xgboost_model_directory\"\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "# Save the XGBoost model locally using joblib\n",
    "joblib.dump(best_xgb_model, f\"{model_dir}/xgboost_model.pkl\")\n",
    "\n",
    "# Upload the model to Hopsworks Model Registry\n",
    "model_meta = mr.python.create_model(\n",
    "    name=\"XGBoost_Model\",\n",
    "    description=\"Best XGBoost model trained with hyperparameter tuning\",\n",
    "    metrics=xgb_metrics,  # Dictionary of evaluation metrics\n",
    "    input_example=X_train_split[0:1]  # Optional: Example of input data\n",
    ")\n",
    "\n",
    "# Upload the model to the registry\n",
    "model_meta.save(f\"{model_dir}/xgboost_model.pkl\")\n",
    "\n",
    "print(f\"Model 'XGBoost_Model' successfully saved to Hopsworks Model Registry.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test_scaled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 16\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Assuming X_test_split and y_test_split are your feature and label datasets\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Define X_test_split, y_test_split (use your existing datasets)\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m X_test_split \u001b[38;5;241m=\u001b[39m \u001b[43mX_test_scaled\u001b[49m  \u001b[38;5;66;03m# Assuming scaled features\u001b[39;00m\n\u001b[0;32m     17\u001b[0m y_test_split \u001b[38;5;241m=\u001b[39m y_test  \u001b[38;5;66;03m# Assuming labels\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Initialize MLflow experiment\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_test_scaled' is not defined"
     ]
    }
   ],
   "source": [
    "#Testing data\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import pickle\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, precision_score, recall_score\n",
    "import hopsworks\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Assuming X_test_split and y_test_split are your feature and label datasets\n",
    "# Define X_test_split, y_test_split (use your existing datasets)\n",
    "X_test_split = X_test_scaled  # Assuming scaled features\n",
    "y_test_split = y_test  # Assuming labels\n",
    "\n",
    "# Initialize MLflow experiment\n",
    "mlflow.set_experiment(\"ML Models with Hyperparameter Tuning\")\n",
    "\n",
    "# Hyperparameter grids for each model\n",
    "logreg_params = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'solver': ['lbfgs', 'liblinear'],\n",
    "    'max_iter': [100, 200]\n",
    "}\n",
    "\n",
    "rf_params = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [None, 10, 15],\n",
    "    'min_samples_split': [2, 3],\n",
    "    'min_samples_leaf': [1, 3]\n",
    "}\n",
    "\n",
    "xgb_params = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 6, 10]\n",
    "}\n",
    "\n",
    "# Function to perform model testing, evaluation, and logging\n",
    "def evaluate_and_log_model(model, param_grid, model_name):\n",
    "    # Start MLflow run\n",
    "    with mlflow.start_run(run_name=model_name):\n",
    "        # GridSearchCV for hyperparameter tuning (already done with training data)\n",
    "        grid_search = GridSearchCV(model, param_grid, cv=3, n_jobs=-1, verbose=2)\n",
    "        grid_search.fit(X_train_split, y_train_split)  # This uses the best parameters from training\n",
    "        \n",
    "        \n",
    "        # Get the best model from GridSearchCV\n",
    "        best_model = grid_search.best_estimator_\n",
    "        \n",
    "        # Evaluate on the testing dataset\n",
    "        y_test_pred = best_model.predict(X_test_split)\n",
    "        test_acc = accuracy_score(y_test_split, y_test_pred)\n",
    "        test_f1 = f1_score(y_test_split, y_test_pred, average='weighted')\n",
    "        test_precision = precision_score(y_test_split, y_test_pred, average='weighted')\n",
    "        test_recall = recall_score(y_test_split, y_test_pred, average='weighted')\n",
    "        \n",
    "        # Log parameters, metrics, and the model with MLflow\n",
    "        mlflow.log_params(grid_search.best_params_)\n",
    "        mlflow.log_metrics({\n",
    "            \"test_accuracy\": test_acc, \n",
    "            \"test_f1_score\": test_f1,\n",
    "            \"test_precision\": test_precision, \n",
    "            \"test_recall\": test_recall\n",
    "        })\n",
    "        mlflow.sklearn.log_model(best_model, model_name)\n",
    "        \n",
    "        # Save the model as a pickle file\n",
    "        pickle_file = f\"./models/{model_name}_model.pkl\"\n",
    "        with open(pickle_file, 'wb') as f:\n",
    "            pickle.dump(best_model, f)\n",
    "        print(f\"Model saved as {model_name}_model.pkl\")\n",
    "        \n",
    "        # Print evaluation results\n",
    "        print(f\"Best Parameters for {model_name}: {grid_search.best_params_}\")\n",
    "        print(f\"Test Accuracy: {test_acc:.4f}, F1 Score: {test_f1:.4f}, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}\")\n",
    "        print(\"Test classification report:\")\n",
    "        print(classification_report(y_test_split, y_test_pred))\n",
    "        \n",
    "        return best_model, {\"accuracy\": test_acc, \"f1_score\": test_f1, \"precision\": test_precision, \"recall\": test_recall}\n",
    "\n",
    "# Evaluate models and get the best models and their metrics\n",
    "best_logistic_model, logistic_metrics = evaluate_and_log_model(LogisticRegression(), logreg_params, \"Logistic_Regression\")\n",
    "best_rf_model, rf_metrics = evaluate_and_log_model(RandomForestClassifier(), rf_params, \"Random_Forest\")\n",
    "best_xgb_model, xgb_metrics = evaluate_and_log_model(XGBClassifier(use_label_encoder=False, eval_metric='logloss'), xgb_params, \"XGBoost\")\n",
    "\n",
    "# Save XGBoost model to Hopsworks\n",
    "# Connect to Hopsworks\n",
    "project = hopsworks.login(api_key_value=\"O4IOxWozstKu0BFQ.07C1tbvgVI5C4XNLbLrGH4PS4t0EqBYN00ex8318TNIkl82WwDi3Vh9MidMrCA83\")\n",
    "\n",
    "# Get the Model Registry\n",
    "mr = project.get_model_registry()\n",
    "\n",
    "# Directory to save the model locally (for later upload to Hopsworks)\n",
    "model_dir = \"xgboost_model_directory\"\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "# Save the XGBoost model locally using joblib\n",
    "joblib.dump(best_xgb_model, f\"{model_dir}/xgboost_model.pkl\")\n",
    "\n",
    "# Upload the model to Hopsworks Model Registry\n",
    "model_meta = mr.python.create_model(\n",
    "    name=\"XGBoost_Model_Test\",\n",
    "    description=\"Best XGBoost model evaluated on test data\",\n",
    "    metrics=xgb_metrics,  # Dictionary of evaluation metrics from the test data\n",
    "    input_example=X_test_split[0:1]  # Optional: Example of input data\n",
    ")\n",
    "\n",
    "# Upload the model to the registry\n",
    "model_meta.save(f\"{model_dir}/xgboost_model.pkl\")\n",
    "\n",
    "print(f\"Model 'XGBoost_Model_Test' successfully saved to Hopsworks Model Registry.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
